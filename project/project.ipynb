{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read titles from disk\n",
    "titles = []\n",
    "with open('clickbait_dataset.txt', 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        titles.append(\"<start> \" + l.strip() + \" <end>\")\n",
    "shuffle(titles, random_state=42)\n",
    "\n",
    "# tokenize the titles\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000,\n",
    "                                                  oov_token='<unk>',\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(titles)\n",
    "train_seqs = tokenizer.texts_to_sequences(titles)\n",
    "\n",
    "# pad the titles to get a numpy array\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "train_seqs_pad = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "rnn_units = 256\n",
    "batch_size = 64\n",
    "num_steps = len(train_seqs) // batch_size\n",
    "max_length = max(len(t) for t in train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_seqs_pad)\n",
    "dataset = dataset.shuffle(1000).batch(batch_size)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "class RNN_Model(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Model, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTMCell(units, recurrent_initializer='glorot_uniform')\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None):\n",
    "        encoded_txt = self.embedding(inputs)\n",
    "        \n",
    "        states = states if states != None else self.lstm.get_initial_state(inputs=encoded_txt)\n",
    "        \n",
    "        outputs, states_nxt = self.lstm(encoded_txt, states)\n",
    "        x = self.dense(outputs)\n",
    "        return x, states_nxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_Model(embedding_dim, rnn_units, num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(model=model,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(title_seq):\n",
    "    loss = 0\n",
    "    states = None\n",
    "    with tf.GradientTape() as tape:\n",
    "        for i in range(title_seq.shape[1]-1):\n",
    "            inputs = title_seq[:, i]\n",
    "            prediction, states = model(inputs, states)\n",
    "            \n",
    "            loss += loss_function(title_seq[:, i+1], prediction)\n",
    "\n",
    "    total_loss = (loss / int(title_seq.shape[1]))\n",
    "\n",
    "    trainable_variables = model.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 0 Loss 1.3055\n",
      "Epoch 12 Batch 100 Loss 1.2575\n",
      "Epoch 12 Batch 200 Loss 1.1534\n",
      "Epoch 12 Loss 1.276757\n",
      "Time taken for 1 epoch 246.0968861579895 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.2494\n",
      "Epoch 13 Batch 100 Loss 1.2787\n",
      "Epoch 13 Batch 200 Loss 1.2048\n",
      "Epoch 13 Loss 1.256216\n",
      "Time taken for 1 epoch 246.00481534004211 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.1709\n",
      "Epoch 14 Batch 100 Loss 1.2421\n",
      "Epoch 14 Batch 200 Loss 1.1669\n",
      "Epoch 14 Loss 1.235014\n",
      "Time taken for 1 epoch 246.1310214996338 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.2308\n",
      "Epoch 15 Batch 100 Loss 1.1437\n",
      "Epoch 15 Batch 200 Loss 1.0828\n",
      "Epoch 15 Loss 1.216074\n",
      "Time taken for 1 epoch 247.85262060165405 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.2200\n",
      "Epoch 16 Batch 100 Loss 1.1892\n",
      "Epoch 16 Batch 200 Loss 1.1073\n",
      "Epoch 16 Loss 1.196643\n",
      "Time taken for 1 epoch 252.4271650314331 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.2199\n",
      "Epoch 17 Batch 100 Loss 1.1517\n",
      "Epoch 17 Batch 200 Loss 1.1120\n",
      "Epoch 17 Loss 1.178150\n",
      "Time taken for 1 epoch 248.58952474594116 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.2537\n",
      "Epoch 18 Batch 100 Loss 1.0358\n",
      "Epoch 18 Batch 200 Loss 1.0499\n",
      "Epoch 18 Loss 1.160843\n",
      "Time taken for 1 epoch 247.92514038085938 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.1083\n",
      "Epoch 19 Batch 100 Loss 1.1311\n",
      "Epoch 19 Batch 200 Loss 1.0502\n",
      "Epoch 19 Loss 1.143150\n",
      "Time taken for 1 epoch 249.06820464134216 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.0865\n",
      "Epoch 20 Batch 100 Loss 1.2021\n",
      "Epoch 20 Batch 200 Loss 1.1458\n",
      "Epoch 20 Loss 1.126324\n",
      "Time taken for 1 epoch 248.59553122520447 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, title_seq) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(title_seq)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(title_seq.shape[1])))\n",
    "\n",
    "    ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_title():\n",
    "    inputs = tf.Variable([tokenizer.word_index['<start>']])\n",
    "    states = None\n",
    "    \n",
    "    result = ['<start>']\n",
    "    for i in range(max_length):\n",
    "        nxt_word, states = model(inputs, states)\n",
    "        \n",
    "        predicted_id = tf.random.categorical(nxt_word, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "        inputs = tf.Variable([predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are you way to not watching this holiday opinions <unk> <unk> we've into the hunter cast <unk>\n",
      "the rock kids just <unk> the chennai rains when she <unk> about taylor swift off\n",
      "you'll only guess which k pop group from 2019 based on which bts you should visit what your crush\n",
      "here are all the <unk> to taylor swift's way laugh the investigation into their lives\n",
      "19 times bbc company was the best romantic on this decade\n",
      "which character from facebook are you\n",
      "an old <unk> shot the <unk> art and anne bush and there are <unk>\n",
      "62 christmas videos and we'll tell you where are 2020\n",
      "i'm a black jackson fan and we'll predict which high school musical as you\n",
      "are you more like never <unk> a college level netflix\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(*gen_new_title()[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
